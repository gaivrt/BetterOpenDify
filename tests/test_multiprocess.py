#!/usr/bin/env python3
"""
å¤šè¿›ç¨‹å¹¶å‘æµ‹è¯• - éªŒè¯ SQLite ConversationMapper çš„å¹¶å‘å®‰å…¨æ€§
"""

import os
import sys
import time
import uuid
import logging
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
import tempfile
import shutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from conversation_mapper_sqlite import ConversationMapper

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - PID:%(process)d - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def worker_task(worker_id: int, db_path: str, operations_per_worker: int):
    """
    å·¥ä½œè¿›ç¨‹ä»»åŠ¡ï¼šæ¨¡æ‹ŸçœŸå®çš„å¹¶å‘æ“ä½œ
    """
    logger.info(f"Worker {worker_id} started with PID {os.getpid()}")
    
    try:
        # æ¯ä¸ªè¿›ç¨‹åˆ›å»ºè‡ªå·±çš„ ConversationMapper å®ä¾‹
        mapper = ConversationMapper(db_path)
        
        results = {
            'worker_id': worker_id,
            'pid': os.getpid(),
            'operations_completed': 0,
            'errors': 0,
            'mapping_conflicts': 0
        }
        
        for i in range(operations_per_worker):
            try:
                # ç”Ÿæˆå”¯ä¸€çš„æ ‡è¯†ç¬¦
                chat_id = f"chat-{worker_id}-{i}-{uuid.uuid4().hex[:8]}"
                conv_id = f"conv-{worker_id}-{i}-{uuid.uuid4().hex[:8]}"
                
                # æµ‹è¯•åŸºæœ¬æ“ä½œ
                # 1. è®¾ç½®æ˜ å°„
                mapper.set_mapping(chat_id, conv_id)
                
                # 2. æ£€æŸ¥æ˜ å°„æ˜¯å¦å­˜åœ¨
                if not mapper.has_mapping(chat_id):
                    results['mapping_conflicts'] += 1
                    logger.warning(f"Worker {worker_id}: Mapping not found immediately after creation")
                
                # 3. è·å–æ˜ å°„
                retrieved_conv_id = mapper.get_dify_conversation_id(chat_id)
                if retrieved_conv_id != conv_id:
                    results['mapping_conflicts'] += 1
                    logger.warning(f"Worker {worker_id}: Retrieved conv_id mismatch: {retrieved_conv_id} != {conv_id}")
                
                # 4. æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
                mapper.update_last_used(chat_id)
                
                # 5. æ¯10æ¬¡æ“ä½œæ£€æŸ¥ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯
                if i % 10 == 0:
                    stats = mapper.get_mapping_stats()
                    logger.debug(f"Worker {worker_id}: Current stats - Total: {stats['total']}")
                
                results['operations_completed'] += 1
                
                # æ¨¡æ‹Ÿä¸€äº›å¤„ç†å»¶è¿Ÿ
                time.sleep(0.001)
                
            except Exception as e:
                results['errors'] += 1
                logger.error(f"Worker {worker_id} operation {i} failed: {e}")
        
        # æœ€ç»ˆç»Ÿè®¡
        final_stats = mapper.get_mapping_stats()
        results['final_mapping_count'] = final_stats['total']
        
        logger.info(f"Worker {worker_id} completed: {results['operations_completed']} operations, {results['errors']} errors")
        return results
        
    except Exception as e:
        logger.error(f"Worker {worker_id} failed: {e}")
        return {
            'worker_id': worker_id,
            'pid': os.getpid(),
            'operations_completed': 0,
            'errors': 1,
            'error_message': str(e)
        }

def test_concurrent_access():
    """æµ‹è¯•å¹¶å‘è®¿é—®"""
    logger.info("=== å¼€å§‹å¹¶å‘è®¿é—®æµ‹è¯• ===")
    
    # åˆ›å»ºä¸´æ—¶æ•°æ®åº“æ–‡ä»¶
    temp_dir = tempfile.mkdtemp(prefix="opendify_test_")
    db_path = os.path.join(temp_dir, "test_conversations.db")
    
    try:
        # æµ‹è¯•å‚æ•°
        num_workers = 4  # è¿›ç¨‹æ•°
        operations_per_worker = 50  # æ¯ä¸ªè¿›ç¨‹çš„æ“ä½œæ¬¡æ•°
        
        logger.info(f"å¯åŠ¨ {num_workers} ä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹æ‰§è¡Œ {operations_per_worker} æ¬¡æ“ä½œ")
        logger.info(f"ä½¿ç”¨æ•°æ®åº“: {db_path}")
        
        start_time = time.time()
        
        # ä½¿ç”¨è¿›ç¨‹æ± æ‰§è¡Œå¹¶å‘æµ‹è¯•
        with ProcessPoolExecutor(max_workers=num_workers) as executor:
            # æäº¤ä»»åŠ¡
            futures = []
            for worker_id in range(num_workers):
                future = executor.submit(
                    worker_task, 
                    worker_id, 
                    db_path, 
                    operations_per_worker
                )
                futures.append(future)
            
            # æ”¶é›†ç»“æœ
            results = []
            for future in as_completed(futures):
                try:
                    result = future.result(timeout=60)  # 60ç§’è¶…æ—¶
                    results.append(result)
                except Exception as e:
                    logger.error(f"Worker failed with exception: {e}")
                    results.append({'error': str(e)})
        
        end_time = time.time()
        
        # åˆ†æç»“æœ
        logger.info("=== æµ‹è¯•ç»“æœåˆ†æ ===")
        total_operations = sum(r.get('operations_completed', 0) for r in results)
        total_errors = sum(r.get('errors', 0) for r in results)
        total_conflicts = sum(r.get('mapping_conflicts', 0) for r in results)
        
        logger.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
        logger.info(f"æ€»æ“ä½œæ•°: {total_operations}")
        logger.info(f"æ€»é”™è¯¯æ•°: {total_errors}")
        logger.info(f"æ˜ å°„å†²çªæ•°: {total_conflicts}")
        logger.info(f"æ“ä½œæˆåŠŸç‡: {(total_operations / (num_workers * operations_per_worker)) * 100:.2f}%")
        
        # éªŒè¯æœ€ç»ˆæ•°æ®ä¸€è‡´æ€§
        final_mapper = ConversationMapper(db_path)
        final_stats = final_mapper.get_mapping_stats()
        expected_total = num_workers * operations_per_worker
        
        logger.info(f"é¢„æœŸæ˜ å°„æ€»æ•°: {expected_total}")
        logger.info(f"å®é™…æ˜ å°„æ€»æ•°: {final_stats['total']}")
        logger.info(f"æ•°æ®ä¸€è‡´æ€§: {'âœ… é€šè¿‡' if final_stats['total'] == expected_total else 'âŒ å¤±è´¥'}")
        
        # æµ‹è¯•æ•°æ®åº“ä¿¡æ¯
        db_info = final_mapper.get_database_info()
        logger.info(f"æ•°æ®åº“å¤§å°: {db_info.get('database_size_bytes', 0)} å­—èŠ‚")
        logger.info(f"æ—¥å¿—æ¨¡å¼: {db_info.get('journal_mode', 'unknown')}")
        
        return {
            'success': total_errors == 0 and total_conflicts == 0,
            'total_operations': total_operations,
            'total_errors': total_errors,
            'total_conflicts': total_conflicts,
            'execution_time': end_time - start_time,
            'data_consistency': final_stats['total'] == expected_total
        }
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            shutil.rmtree(temp_dir)
            logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_dir}")
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

def test_database_features():
    """æµ‹è¯•æ•°æ®åº“ç‰¹æ€§"""
    logger.info("=== æ•°æ®åº“ç‰¹æ€§æµ‹è¯• ===")
    
    temp_dir = tempfile.mkdtemp(prefix="opendify_feature_test_")
    db_path = os.path.join(temp_dir, "feature_test.db")
    
    try:
        mapper = ConversationMapper(db_path)
        
        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        test_chat_id = "test-chat-123"
        test_conv_id = "test-conv-456"
        
        # è®¾ç½®æ˜ å°„
        mapper.set_mapping(test_chat_id, test_conv_id)
        logger.info("âœ… è®¾ç½®æ˜ å°„æˆåŠŸ")
        
        # è·å–æ˜ å°„
        retrieved = mapper.get_dify_conversation_id(test_chat_id)
        assert retrieved == test_conv_id, f"æ˜ å°„ä¸åŒ¹é…: {retrieved} != {test_conv_id}"
        logger.info("âœ… è·å–æ˜ å°„æˆåŠŸ")
        
        # æ£€æŸ¥æ˜ å°„å­˜åœ¨
        assert mapper.has_mapping(test_chat_id), "æ˜ å°„åº”è¯¥å­˜åœ¨"
        logger.info("âœ… æ˜ å°„å­˜åœ¨æ£€æŸ¥æˆåŠŸ")
        
        # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
        mapper.update_last_used(test_chat_id)
        logger.info("âœ… æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´æˆåŠŸ")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = mapper.get_mapping_stats()
        assert stats['total'] == 1, f"ç»Ÿè®¡ä¸æ­£ç¡®: {stats['total']} != 1"
        logger.info(f"âœ… ç»Ÿè®¡ä¿¡æ¯æ­£ç¡®: {stats}")
        
        # è·å–æœ€è¿‘æ˜ å°„
        recent = mapper.get_recent_mappings(5)
        assert len(recent) == 1, f"æœ€è¿‘æ˜ å°„æ•°é‡ä¸æ­£ç¡®: {len(recent)} != 1"
        logger.info(f"âœ… æœ€è¿‘æ˜ å°„åŠŸèƒ½æ­£å¸¸: {len(recent)} æ¡è®°å½•")
        
        # æµ‹è¯•æ¸…ç†åŠŸèƒ½
        time.sleep(1)  # ç­‰å¾…1ç§’
        cleaned = mapper.cleanup_old_mappings(max_age_days=0)  # æ¸…ç†æ‰€æœ‰
        assert cleaned == 1, f"æ¸…ç†æ•°é‡ä¸æ­£ç¡®: {cleaned} != 1"
        logger.info("âœ… æ¸…ç†åŠŸèƒ½æ­£å¸¸")
        
        # éªŒè¯æ¸…ç†åçš„çŠ¶æ€
        final_stats = mapper.get_mapping_stats()
        assert final_stats['total'] == 0, f"æ¸…ç†åç»Ÿè®¡ä¸æ­£ç¡®: {final_stats['total']} != 0"
        logger.info("âœ… æ¸…ç†åçŠ¶æ€æ­£ç¡®")
        
        # æµ‹è¯•æ•°æ®åº“ä¼˜åŒ–
        mapper.optimize_database()
        logger.info("âœ… æ•°æ®åº“ä¼˜åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®åº“ä¿¡æ¯
        db_info = mapper.get_database_info()
        assert 'database_size_bytes' in db_info, "æ•°æ®åº“ä¿¡æ¯åº”åŒ…å«å¤§å°"
        assert 'journal_mode' in db_info, "æ•°æ®åº“ä¿¡æ¯åº”åŒ…å«æ—¥å¿—æ¨¡å¼"
        logger.info(f"âœ… æ•°æ®åº“ä¿¡æ¯è·å–æˆåŠŸ: {db_info}")
        
        logger.info("=== æ‰€æœ‰æ•°æ®åº“ç‰¹æ€§æµ‹è¯•é€šè¿‡ ===")
        return True
        
    except Exception as e:
        logger.error(f"æ•°æ®åº“ç‰¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    finally:
        try:
            shutil.rmtree(temp_dir)
        except Exception as e:
            logger.warning(f"æ¸…ç†æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¼€å§‹ OpenDify SQLite ConversationMapper å¹¶å‘å®‰å…¨æµ‹è¯•")
    
    # æµ‹è¯•æ•°æ®åº“åŸºæœ¬ç‰¹æ€§
    logger.info("\n" + "="*50)
    feature_test_passed = test_database_features()
    
    if not feature_test_passed:
        logger.error("æ•°æ®åº“ç‰¹æ€§æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡å¹¶å‘æµ‹è¯•")
        return False
    
    # æµ‹è¯•å¹¶å‘è®¿é—®å®‰å…¨æ€§
    logger.info("\n" + "="*50)
    concurrent_result = test_concurrent_access()
    
    # æœ€ç»ˆæ€»ç»“
    logger.info("\n" + "="*50)
    logger.info("=== æµ‹è¯•æ€»ç»“ ===")
    
    if feature_test_passed and concurrent_result['success'] and concurrent_result['data_consistency']:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼SQLite ConversationMapper å¯ä»¥å®‰å…¨ç”¨äºå¤šè¿›ç¨‹ç¯å¢ƒ")
        logger.info("ä¸»è¦æ”¹è¿›:")
        logger.info("  âœ… ä½¿ç”¨ SQLite æ•°æ®åº“æ›¿ä»£ JSON æ–‡ä»¶å­˜å‚¨")
        logger.info("  âœ… æ•°æ®åº“è‡ªåŠ¨å¤„ç†å¹¶å‘è®¿é—®å’Œäº‹åŠ¡")
        logger.info("  âœ… WAL æ¨¡å¼æé«˜å¹¶å‘æ€§èƒ½")
        logger.info("  âœ… ç§»é™¤äº†ä¸å¿…è¦çš„çº¿ç¨‹é”")
        logger.info("  âœ… æ”¯æŒå¤šè¿›ç¨‹å®‰å…¨è®¿é—®")
        return True
    else:
        logger.error("âŒ æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)